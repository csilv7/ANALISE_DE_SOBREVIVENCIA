---
title: "Análise de Sobrevivência - Modelos AFT"
subtitle: "em Dados sujeitos à Censura Intervalar"
author: "Breno C R Silva"
format:
  html:
    #theme: "lux"
    #fontcolor: "black"
    number-sections: true
    code-copy: true
    toc: true
    toc-title: Índice
    
lang: pt
---

```{r, include=FALSE}
options(OutDec = ",", digits = 6)
```

# Modelagem de Dados Sujeitos à Censura Intervalar

## Exemplo Simulado

...

## Exemplo do Livro

Será ajustado um modelo de sobrevivência com dados do estudo de câncer de mama apresentado na Seção 8.2.1 do livro Análise de Sobrevivência Aplicada de Colosimo e Giolo (2006).

```{r}
#| tbl-cap: "Dez Primeiras Observações dos Dados do Estudo de Câncer de Mama."
#| label: tbl-SIMULexpAFT
#| tbl-cap-location: top

# Carregamento dos dados
breast <- read.table("https://docs.ufpr.br/~giolo/asa/dados/breast.txt", h = TRUE)
breast$left <- ifelse(breast$left == 0, 0.01, breast$left)
breast$right <- ifelse(is.na(breast$right), Inf, breast$right)

# Tabela
knitr::kable(
  head(breast, 10),
  col.names = c("$L$", "$U$", "Ther", "Cens"),
  escape = FALSE,
  align = "c",
  booktabs = TRUE
)
```

### Ajuste via Modelo Weibull

1. Ajuste via Pacote `survival`:

```{r}
# Chamando o Pacote
library(survival)

# Ajustando o modelo
ajust.surv <- survreg(Surv(left, right, type = "interval2") ~ ther, breast, dist = "weibull")

# Visualização do ajuste
summary(ajust.surv)
```

2. Ajuste via Função `optim`:

```{r, message=FALSE, warning=FALSE}
# Para o Tempo de Falha não Censurado
surv.interval <- function(L, R, x.cov, coeffs.par, shape.par, scale.par) {
  # Preditor Linear
  effect <- x.cov %*% coeffs.par
  
  # Contribuição para Função Verossimilhança
  surv.L <- 1-pweibull(q = L/exp(effect), shape = shape.par, scale = scale.par)
  surv.R <- 1-pweibull(q = R/exp(effect), shape = shape.par, scale = scale.par)
  return(surv.L - surv.R)
}

# Para o Tempo de Falha Censurado
surv.cens <- function(L, x.cov, coeffs.par, shape.par, scale.par) {
  # Preditor Linear
  effect <- x.cov %*% coeffs.par
  
  # Contribuição para Função Verossimilhança
  surv.L <- 1-pweibull(q = L/exp(effect), shape = shape.par, scale = scale.par)
  return(surv.L)
}

# Função Log-verossimilhança
loglikelihood <- function(par, interval, cens, X.cov) {
  # Número de parâmetros
  n.par <- length(par)
  
  # Distinção de parâmetros
  shape.par <- par[1]
  scale.par <- exp(par[2])
  coeffs.par <- par[3:n.par]
  
  # Distinção dos Limites
  L <- interval[, 1]
  R <- interval[, 2]
  
  # Função Log-verossimilhança
  S1 <- surv.interval(L = L, R = R, x.cov = X.cov, coeffs.par = coeffs.par,
                      shape.par = shape.par, scale.par = scale.par)
  S2 <- surv.cens(L = L, x.cov = X.cov, coeffs.par = coeffs.par,
                  shape.par = shape.par, scale.par = scale.par)
  flv <- sum(cens * log(S1) + (1 - cens) * log(S2))
  return(-flv)
  
}

# Separação dos Dados
y <- cbind(breast$left, breast$right)/12 # Variável Resposta
X <- cbind(breast$ther)               # Matrix de Covariáveis

# Número de parâmetros para estimar
n.par <- 2 + ncol(X)

# Chute Inicial
init <- rep(1, n.par)

# Maximização
ajust.optim <- optim(par=init, fn = loglikelihood, 
                     gr = NULL, method = "BFGS", hessian = TRUE, 
                     interval=y, cens=breast$cens, X.cov=X)

# Visualização do Ajuste
ajust.optim
```

### Ajuste via Modelo Modelo Exponencial por Partes de Potência

1. Ajuste via Pacote `eha`:

```{r, message=FALSE, warning=FALSE}
# Chamando o Pacote
library(eha)
```

2. Ajuste via Função `optim`:

O objetivo é usar a função de otimização do **R** para maximizar a função $$\ell(\theta) = \sum_{i=1}^{n} \delta_{i} \ln{\left[ S(l_{i} | \mathbf{x}) - S(u_{i} | \mathbf{x}) \right]} + (1-\delta_{i}) \ln{\left[ S(l_{i} | \mathbf{x}) \right]}.$$

```{r}
# Para o Tempo de Falha não Censurado
surv.interval <- function(L, R, x.cov, coeffs.par, cuts.points, rates.par, power.par) {
  # Preditor linear
  effect <- x.cov %*% coeffs.par
  
  # Contribuição para Função Verossimilhança
  surv.L <- 1 - ppch(q = L/exp(effect), cuts = cuts.points, levels = rates.par)^power.par
  surv.R <- 1 - ppch(q = R/exp(effect), cuts = cuts.points, levels = rates.par)^power.par
  return(surv.L - surv.R)
}
# Para o Tempo de Falha Censurado
surv.cens <- function(L, x.cov, coeffs.par, cuts.points, rates.par, power.par) {
  # Preditor Linear
  effect <- x.cov %*% coeffs.par
  
  # Contribuição para Função Verossimilhança
  surv.L <- 1 - ppch(q = L/exp(effect), cuts = cuts.points, levels = rates.par)^power.par
  return(surv.L)
}

# Função Log-verossimilhança
loglikelihood <- function(par, interval, cens, X, cuts.points) {
  # Número de parâmetros e partições
  n.par <- length(par)
  n.cuts <- length(cuts.points)
  
  # Distinção de parâmetros
  rates.par <- exp(par[1:(n.cuts+1)])
  power.par <- par[n.cuts+2]
  coeffs.par <- par[(n.cuts + 3):n.par]
  
  # Distinção dos Limites
  L <- interval[, 1]
  R <- interval[, 2]
  
 
  # Função Log-verossimilhança
  S1 <- surv.interval(L=L, R=R, x.cov=X, coeffs.par=coeffs.par,
                      cuts.points=cuts.points, rates.par=rates.par,
                      power.par=power.par)
  S2 <- surv.cens(L=L, x.cov=X, coeffs.par=coeffs.par,
                  cuts.points=cuts.points, rates.par=rates.par,
                  power.par=power.par)
  flv <- sum(cens * log(S1) + (1 - cens) * log(S2))
  return(-flv)
}
```

```{r}
time.grid.interval <- function(li=li, ri=ri, type=type, bmax=bmax)
{
  ## FunÃ§Ã£o que retorna os intervalos da partiÃ§Ã£o mais fina
  ## baseada nos limites observados, distintos e finitos.
  ## Argumentos:
  ## li: limite inferior dos intervalos observados.
  ## ri: limite superior dos intervalos observados.
  ## bmax: numero mÃ¡ximo de intervalos.

  #--- Inicio da funÃ§Ã£o:

  #-- Construir uma grade tipo 1:
  if(type=="OBS")
  {
    #grid.vet <- sort(unique(c(0, li, ri, Inf)))
    grid.vet <- sort(unique(c(0, li, is.finite(ri), Inf)))
    grid.size.vet <- length(grid.vet) # Grid time size

    if( isTRUE(bmax<grid.size.vet)==TRUE )
    {
      k        <- round((length(grid.vet)-1)/bmax,0)
      id.grid  <- round(seq(k,(length(grid.vet)-1), length.out=bmax),0)
      grid.vet <- c(0,grid.vet[-1][id.grid])
      return(grid.vet)
    }else{
      grid.vet <- sort(unique(c(0, li, ri, Inf)))
      return(grid.vet)
    }
  }
  if(type=="EQUI")
  {
    grade.vet <- seq(0, max(ri[ri!=Inf]), length.out=bmax)
    grid.vet <- c(grade.vet,Inf)
    return(grid.vet)
  }
}
```

```{r}
y <- cbind(breast$left, breast$right) / 12 
cuts.grid <- time.grid.interval(li=y[, 1], ri=y[, 2], type="OBS", bmax = 2)
cuts.grid <- cuts.grid[c(-1, -length(cuts.grid))]
cuts.grid
```



```{r}
# Separação dos Dados
y <- cbind(breast$left, breast$right)/12 # Variável Resposta
X <- cbind(breast$ther)                  # Matrix de Covariáveis
cuts <- cuts.grid                        # Pontos de Corte

# Número de parâmetros para estimar
n.par <- length(cuts) + 2 + ncol(X)

# Chute Inicial
init <- rep(1, n.par)

# Maximização
ajust.optim <- optim(par=init, fn = loglikelihood, 
                     gr = NULL, method = "BFGS", hessian = FALSE, 
                     interval=y, cens=breast$cens, X=X, cuts.points=cuts)

# Visualização do Ajuste
ajust.optim
```